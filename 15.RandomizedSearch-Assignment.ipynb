{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3.\tImplement strategies such as oversampling, undersampling, or using weighted loss functions to handle class imbalances. Implement a hyperparameter optimization pipeline for your MLP model using techniques like grid search or random search.\n",
    "a.\tExplore the impact of varying hyperparameters (e.g., learning rate, number of hidden layers, batch size) on model performance.\n",
    "b.\tCompare the training time and performance of your GPU-accelerated implementation with the CPU-based version."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.datasets import load_breast_cancer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "import time\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense\n",
    "from sklearn.metrics import accuracy_score as accuracy, classification_report\n",
    "\n",
    "dataset = load_breast_cancer()\n",
    "\n",
    "data = pd.DataFrame(dataset.data, columns = dataset.feature_names)\n",
    "data['class'] = dataset.target\n",
    "\n",
    "count_class_1, count_class_0 = data['class'].value_counts()\n",
    "data_class_0 = data[data['class'] == 0]\n",
    "data_class_1 = data[data['class'] == 1]\n",
    "\n",
    "data_class_1_under = data_class_1.sample(count_class_0)\n",
    "dataset_under = pd.concat([data_class_0, data_class_1_under], axis = 0)\n",
    "X_under = dataset_under.drop(\"class\", axis = 'columns')\n",
    "y_under = dataset_under['class']\n",
    "X_under_train, X_under_test, y_under_train, y_under_test = train_test_split(X_under, y_under, test_size=0.2, stratify=y_under)\n",
    "\n",
    "\n",
    "data_class_0_over = data_class_0.sample(count_class_1, replace = True)\n",
    "dataset_over = pd.concat([data_class_0_over, data_class_1], axis = 0)\n",
    "X_over = dataset_over.drop(\"class\", axis = 'columns')\n",
    "y_over = dataset_over['class']\n",
    "X_over_train, X_over_test, y_over_train, y_over_test = train_test_split(X_over, y_over, test_size=0.2, stratify=y_over)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "RANDOMIZED SEARCH CV (UNDER SAMPLING)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU PERFORMANCE:-\n",
      "Accuracy:-  0.8941176470588236\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.84      0.98      0.90        42\n",
      "           1       0.97      0.81      0.89        43\n",
      "\n",
      "    accuracy                           0.89        85\n",
      "   macro avg       0.90      0.90      0.89        85\n",
      "weighted avg       0.91      0.89      0.89        85\n",
      "\n",
      "CPU Training Time:- 0.11s\n",
      "GPU not available\n"
     ]
    }
   ],
   "source": [
    "mlp = MLPClassifier(max_iter=500)\n",
    "\n",
    "hyperparameters = {\n",
    "    'hidden_layer_sizes': [(20,), (50, 100), (20, 50, 70)],\n",
    "    'learning_rate_init': [0.01, 0.001, 0.0001],\n",
    "    'batch_size': [16, 32, 64]\n",
    "}\n",
    "\n",
    "RandomizedSearch = RandomizedSearchCV(mlp, hyperparameters, cv=5, n_jobs=-1)\n",
    "start_time = time.time()\n",
    "RandomizedSearch.fit(X_under_train, y_under_train)\n",
    "end_time = time.time()\n",
    "\n",
    "best_parameters = RandomizedSearch.best_params_\n",
    "\n",
    "mlp_cpu = MLPClassifier(**best_parameters, max_iter=100)\n",
    "start_time_cpu = time.time()\n",
    "mlp_cpu.fit(X_under_train, y_under_train)\n",
    "end_time_cpu = time.time()\n",
    "\n",
    "predictions_cpu = mlp_cpu.predict(X_under_test)\n",
    "accuracy_cpu = accuracy(y_under_test, predictions_cpu)\n",
    "print(\"CPU PERFORMANCE:-\")\n",
    "print(\"Accuracy:- \", accuracy_cpu)\n",
    "print(classification_report(y_under_test, predictions_cpu))\n",
    "\n",
    "print(f\"CPU Training Time:- {end_time_cpu - start_time_cpu:.2f}s\")\n",
    "\n",
    "if tf.test.is_gpu_available():\n",
    "    tf.keras.backend.clear_session()\n",
    "\n",
    "    X_train = tf.constant(X_under_train, dtype = tf.float32)\n",
    "    y_train = tf.constant(y_under_train, dtype = tf.float32)\n",
    "    X_test = tf.constant(X_under_test, dtype = tf.float32)\n",
    "\n",
    "    mlp_gpu = Sequential([\n",
    "        Dense(80, activation='relu', input_shape= (X_train.shape[1],)),\n",
    "        Dense(40, activation='relu'),\n",
    "        Dense(1, activation='sigmoid')\n",
    "    ])\n",
    "\n",
    "    mlp_gpu.compile(optimizer=tf.keras.Adam(learning_rate = best_parameters['learning_rate)init']),\n",
    "                    loss = 'binary_crossentropy',\n",
    "                    metrics = ['accuracy'])\n",
    "    \n",
    "    start_time_gpu = time.time()\n",
    "    mlp_gpu.fit(X_train, y_train, epochs = 200, batch_size = bet_parametera['batch_size'], verbose=0)\n",
    "    ens_time_gpu = time.time()\n",
    "\n",
    "    predictions_gpu = mlp_gpu.predict(X_test)\n",
    "    predictions_gpu = np.round(prediction_gpu).flatten().astype(int)\n",
    "    accuracy_gpu = accuracy(y_under_test, predictions_gpu)\n",
    "    print(\"GPU PERFORMANCE:- \")\n",
    "    print(\"Accuracy:- \", accuracy_gpu)\n",
    "    print(classification_report(y_under_test, predictions_gpu))\n",
    "    print(f\"Training Time:- {end_time_gpu - start_time_gpu:2.f}s\")\n",
    "\n",
    "else:\n",
    "    print(\"GPU not available\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "RANDOMIZED SEARCH CV (OVER SAMPLING)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU PERFORMANCE:-\n",
      "Accuracy:-  0.9370629370629371\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.96      0.92      0.94        72\n",
      "           1       0.92      0.96      0.94        71\n",
      "\n",
      "    accuracy                           0.94       143\n",
      "   macro avg       0.94      0.94      0.94       143\n",
      "weighted avg       0.94      0.94      0.94       143\n",
      "\n",
      "CPU Training Time:- 0.26s\n",
      "GPU not available\n"
     ]
    }
   ],
   "source": [
    "mlp = MLPClassifier(max_iter=500)\n",
    "\n",
    "hyperparameters = {\n",
    "    'hidden_layer_sizes': [(20,), (50, 100), (20, 50, 70)],\n",
    "    'learning_rate_init': [0.01, 0.001, 0.0001],\n",
    "    'batch_size': [16, 32, 64]\n",
    "}\n",
    "\n",
    "RandomizedSearch = RandomizedSearchCV(mlp, hyperparameters, cv=5, n_jobs=-1)\n",
    "start_time = time.time()\n",
    "RandomizedSearch.fit(X_over_train, y_over_train)\n",
    "end_time = time.time()\n",
    "\n",
    "best_parameters = RandomizedSearch.best_params_\n",
    "\n",
    "mlp_cpu = MLPClassifier(**best_parameters, max_iter=100)\n",
    "start_time_cpu = time.time()\n",
    "mlp_cpu.fit(X_over_train, y_over_train)\n",
    "end_time_cpu = time.time()\n",
    "\n",
    "predictions_cpu = mlp_cpu.predict(X_over_test)\n",
    "accuracy_cpu = accuracy(y_over_test, predictions_cpu)\n",
    "print(\"CPU PERFORMANCE:-\")\n",
    "print(\"Accuracy:- \", accuracy_cpu)\n",
    "print(classification_report(y_over_test, predictions_cpu))\n",
    "\n",
    "print(f\"CPU Training Time:- {end_time_cpu - start_time_cpu:.2f}s\")\n",
    "\n",
    "if tf.test.is_gpu_available():\n",
    "    tf.keras.backend.clear_session()\n",
    "\n",
    "    X_train = tf.constant(X_over_train, dtype = tf.float32)\n",
    "    y_train = tf.constant(y_over_train, dtype = tf.float32)\n",
    "    X_test = tf.constant(X_over_test, dtype = tf.float32)\n",
    "\n",
    "    mlp_gpu = Sequential([\n",
    "        Dense(80, activation='relu', input_shape= (X_train.shape[1],)),\n",
    "        Dense(40, activation='relu'),\n",
    "        Dense(1, activation='sigmoid')\n",
    "    ])\n",
    "\n",
    "    mlp_gpu.compile(optimizer=tf.keras.Adam(learning_rate = best_parameters['learning_rate)init']),\n",
    "                    loss = 'binary_crossentropy',\n",
    "                    metrics = ['accuracy'])\n",
    "    \n",
    "    start_time_gpu = time.time()\n",
    "    mlp_gpu.fit(X_train, y_train, epochs = 200, batch_size = bet_parametera['batch_size'], verbose=0)\n",
    "    ens_time_gpu = time.time()\n",
    "\n",
    "    predictions_gpu = mlp_gpu.predict(X_test)\n",
    "    predictions_gpu = np.round(prediction_gpu).flatten().astype(int)\n",
    "    accuracy_gpu = accuracy(y_overr_test, predictions_gpu)\n",
    "    print(\"GPU PERFORMANCE:- \")\n",
    "    print(\"Accuracy:- \", accuracy_gpu)\n",
    "    print(classification_report(y_over_test, predictions_gpu))\n",
    "    print(f\"Training Time:- {end_time_gpu - start_time_gpu:2.f}s\")\n",
    "\n",
    "else:\n",
    "    print(\"GPU not available\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
