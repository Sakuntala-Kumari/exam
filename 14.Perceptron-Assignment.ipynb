{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2.\tConsider a 2-dimensional data set in which all points with x1 > x2 belong to the positive class, and all points with x1 â‰¤ x2 belong to the negative class. Therefore, the true separator of the two classes is linear hyperplane (line) defined by x1 x2 = 0. Now create a training data set with 20 points randomly generated inside the unit square in the positive quadrant. Label each point depending on whether or not the first coordinate x1 is greater than its second coordinate x2.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import accuracy_score as accuracy\n",
    "\n",
    "def generate_data(num_samples):\n",
    "    np.random.seed(0)\n",
    "    x1 = np.random.randint(1, 100, num_samples)\n",
    "    x2 = np.random.randint(1, 100, num_samples)\n",
    "    y = [1 if i-j>0 else -1 for i, j in zip(x1, x2)]\n",
    "\n",
    "    dataset = np.column_stack((x1, x2, y))\n",
    "\n",
    "    columns = ['X1', 'X2', 'Y']\n",
    "    dataset = pd.DataFrame(data=dataset, columns=columns)\n",
    "\n",
    "    X = dataset[['X1', 'X2']]\n",
    "    Y = dataset['Y']\n",
    "\n",
    "    X = X.values\n",
    "    Y = Y.values\n",
    "    return X, Y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "a.\tImplement the perceptron algorithm without regularization, train it on the 20 points above, and test its accuracy on 1000 randomly generated points inside the unit square. Generate the test points using the same procedure as the training points."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Perceptron:\n",
    "    def __init__(self, learning_rate=0.01, epochs=1000):\n",
    "        self.lr = learning_rate\n",
    "        self.epochs = epochs\n",
    "        self.weights = None\n",
    "        self.bias = None\n",
    "    \n",
    "    def pre_activation(self, x, w, b):\n",
    "        return np.dot(x, w) + b\n",
    "    \n",
    "    def activation(self, x):\n",
    "        return np.where(x >= 0, 1, -1)\n",
    "    \n",
    "    def fit(self, X, y):\n",
    "        num_samples, num_features = X.shape\n",
    "        self.weights = np.zeros(num_features)\n",
    "        self. bias = 0\n",
    "\n",
    "        converged = False\n",
    "        epoch = 0\n",
    "        error = 0\n",
    "\n",
    "        while not converged:\n",
    "            for index, x_i in enumerate(X):\n",
    "                y_predicted = self.activation(self.pre_activation(x_i, self.weights, self.bias))\n",
    "                error = y[index] - y_predicted\n",
    "                self.weights += self.lr * error * x_i\n",
    "                self.bias += self.lr*error\n",
    "\n",
    "            epoch += 1\n",
    "            converged = error == 0 or epoch == self.epochs\n",
    "                \n",
    "    def predict(self, X):\n",
    "        y_predicted = self.activation(self.pre_activation(X, self.weights, self.bias))\n",
    "        return y_predicted\n",
    "\n",
    "Model = Perceptron()\n",
    "num_samples = 20\n",
    "X_train, Y_train = generate_data(num_samples)\n",
    "training_predictions = Model.fit(X_train, Y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy without Regularization =  0.939\n"
     ]
    }
   ],
   "source": [
    "num_samples = 1000\n",
    "X_test, Y_test = generate_data(num_samples)\n",
    "\n",
    "predictions = Model.predict(X_test)\n",
    "print(\"Accuracy without Regularization = \", accuracy(predictions, Y_test))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " b.\tChange the perceptron criterion to hinge-loss in your implementation for training, and repeat the accuracy computation on the same test points above. Regularization is not used."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy with Hinge Loss =  0.939\n"
     ]
    }
   ],
   "source": [
    "class Perceptron:\n",
    "    def __init__(self, learning_rate=0.01, epochs=1000):\n",
    "        self.lr = learning_rate\n",
    "        self.epochs = epochs\n",
    "        self.weights = None\n",
    "        self.bias = None\n",
    "    \n",
    "    def pre_activation(self, x, w, b):\n",
    "        return np.dot(x, w) + b\n",
    "    \n",
    "    def activation(self, x):\n",
    "        return np.where(x >= 0, 1, -1)\n",
    "    \n",
    "    def fit(self, X, y):\n",
    "        num_samples, num_features = X.shape\n",
    "        self.weights = np.zeros(num_features)\n",
    "        self. bias = 0\n",
    "        \n",
    "        converged = False\n",
    "        epoch = 0\n",
    "        hinge_loss = 0\n",
    "        while not converged:\n",
    "            for index, x_i in enumerate(X):\n",
    "                y_predicted = self.activation(self.pre_activation(x_i, self.weights, self.bias))\n",
    "                hinge_loss = max(0, 1 - y[index] * y_predicted) # hinge_loss = max(0, 1-y*f(x)) \n",
    "                \n",
    "                if hinge_loss > 0:\n",
    "                    self.weights += self.lr * y[index] * x_i\n",
    "                    self.bias += self.lr * y[index]\n",
    "            epoch+=1\n",
    "            converged = hinge_loss == 0 or epoch == self.epochs\n",
    "                \n",
    "    def predict(self, X):\n",
    "        y_predicted = self.activation(self.pre_activation(X, self.weights, self.bias))\n",
    "        return y_predicted\n",
    "\n",
    "Model = Perceptron()\n",
    "training_predictions = Model.fit(X_train, Y_train)\n",
    "predictions = Model.predict(X_test)\n",
    "print(\"Accuracy with Hinge Loss = \", accuracy(predictions, Y_test))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
